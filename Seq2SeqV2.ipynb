{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2SeqV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0X3AuKNXqGGv1yXO7mxD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arsuh/Seq2Seq-Chatbot/blob/master/Seq2SeqV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr-v_XT5rDmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8d3u8JprJCI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Arsuh/Seq2Seq-Chatbot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC0ecGJgrdE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import bigquery\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hFdbi55rUvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd Seq2Seq-Chatbot/\n",
        "from Vocabulary import Vocabulary\n",
        "from MainModel import loss_fnc\n",
        "from helper import *\n",
        "from evaluate import evaluate\n",
        "\n",
        "drive_main_path = '/content/drive/My Drive/Colab Files/Chatbot/'\n",
        "main_path = '/content/Seq2Seq-Chatbot/'\n",
        "hparams_path = main_path + 'hyper_parameters_std.json'\n",
        "#hparams_path = main_path + 'hyper_parameters_test.json'\n",
        "ckpt_path = drive_main_path + 'ckeckpoints/'\n",
        "ckpt_prefix = os.path.join(ckpt_path, 'ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYnbqtCuoxod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = ['Hello!',\n",
        "                  'How are you?',\n",
        "                  'Tomorow is my birthday, but I keep feeling sad...',\n",
        "                  'What is your name sir?',\n",
        "                  'Artificial intelligence will take over the world some day!',\n",
        "                  'Can you please bring me some water?',\n",
        "                  'Come on! This is the easiest thing you are supposed to do!',\n",
        "                  'My name is Thomas!']\n",
        "\n",
        "def train_step(hparams, inp, tar, enc_h1, enc_h2):\n",
        "    global enc, dec, opt\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_out, enc_h1, enc_h2 = enc(inp, enc_h1, enc_h2)\n",
        "        dec_h1, dec_h2 = enc_h1, enc_h2\n",
        "        dec_inp = tf.expand_dims([1]*hparams['BATCH_SIZE'], 1)\n",
        "\n",
        "        for t in range(1, tar.shape[1]):\n",
        "            pred, dec_h1, dec_h2, _ = dec(dec_inp, enc_out, dec_h1, dec_h2)\n",
        "\n",
        "            loss += loss_fnc(tar[:, t], pred)\n",
        "            dec_inp = tf.expand_dims(tar[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss/int(tar.shape[1]))\n",
        "    variables = enc.trainable_variables + dec.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    opt.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "def train(hparams, credentials, print_step, offset=0, initial_epoch=1, saving=True, checkpoint_prefix=ckpt_prefix, verbose=True):\n",
        "    global enc, dec, opt\n",
        "    start = time.time()\n",
        "    v = Vocabulary(max_len=hparams['MAX_LEN'])\n",
        "    v.load_bigquery_vocab_from_indexed(credentials, hparams['VOCAB_DB'], hparams['VOCAB'], verbose)\n",
        "    v.create_inputs_from_indexed(credentials,\n",
        "                                 offset=offset,\n",
        "                                 limit_main=hparams['NUM_EXAMPLES'],\n",
        "                                 verbose=True)  # <--- False\n",
        "\n",
        "    if verbose: print('Vocabulary created!')\n",
        "    dataset = create_dataset(v, hparams['BATCH_SIZE'], hparams['NUM_EXAMPLES'])\n",
        "    if verbose: print('Time to initialize model {:.2f} min | {:.2f} hrs\\n'.format((time.time()-start)/60, (time.time()-start)/3600))\n",
        "    del start\n",
        "\n",
        "    if hparams['NUM_EXAMPLES'] == None:\n",
        "        N_BATCH = hparams['MAX_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "    else:\n",
        "        N_BATCH = hparams['NUM_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "\n",
        "    if saving: checkpoint = tf.train.Checkpoint(optimizer=opt, encoder=enc, decoder=dec)\n",
        "\n",
        "    plt_loss = []\n",
        "    for epoch in range(initial_epoch, hparams['EPOCHS']+1):\n",
        "        epoch_time = time.time()\n",
        "        h1, h2 = enc.initialize_hidden()\n",
        "\n",
        "        total_loss = 0\n",
        "        for (batch, (inp, tar)) in enumerate(dataset.take(N_BATCH)):\n",
        "            batch_time = time.time()\n",
        "            batch_loss = train_step(hparams, inp, tar, h1, h2)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % print_step == 0 or batch == 0:\n",
        "              print('  >>> Epoch: {} | Batch: {}\\\\{} | Loss: {:.4f} | Time: {:.2f} sec'\n",
        "                  .format(epoch, batch+1, N_BATCH, batch_loss, time.time() - batch_time))\n",
        "\n",
        "        sentences = random.choices(test_sentences, k=2)\n",
        "        result1, text1, _ = evaluate(sentences[0], v, enc, dec, hparams['MAX_LEN'])\n",
        "        result2, text2, _ = evaluate(sentences[1], v, enc, dec, hparams['MAX_LEN'])\n",
        "        print(50*'+')\n",
        "        print(text1)\n",
        "        print(result1)\n",
        "        print(text2)\n",
        "        print(result2)\n",
        "        print(50*'+')\n",
        "\n",
        "        if saving:\n",
        "            print('Saving model...')\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        plt_loss.append(total_loss/N_BATCH)\n",
        "        print('Epoch: {} | Loss: {:.4f} | Time: {:.2f} min'.format(epoch, total_loss/N_BATCH, (time.time()-epoch_time)/60))\n",
        "    return plt_loss\n",
        "\n",
        "def multi_initializer_train(hparams, credentials, print_step, initial_epoch=1, saving=True, checkpoint_prefix=ckpt_prefix, verbose=True):\n",
        "    global enc, dec, opt\n",
        "    v = Vocabulary(max_len=hparams['MAX_LEN'])\n",
        "    v.load_bigquery_vocab_from_indexed(credentials, hparams['VOCAB_DB'], hparams['VOCAB'], verbose)\n",
        "    if verbose: print('Vocabulary created!')\n",
        "\n",
        "    if hparams['NUM_EXAMPLES'] == None:\n",
        "        N_BATCH = hparams['MAX_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "    else:\n",
        "        N_BATCH = hparams['NUM_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "\n",
        "    if saving: checkpoint = tf.train.Checkpoint(optimizer=opt, encoder=enc, decoder=dec)\n",
        "\n",
        "    plt_loss = []\n",
        "    for epoch in range(initial_epoch, hparams['EPOCHS']+1):\n",
        "        epoch_time = time.time()\n",
        "        ep_losses = []\n",
        "        h1, h2 = enc.initialize_hidden()\n",
        "\n",
        "        reps = int(hparams['MAX_EXAMPLES']//hparams['NUM_EXAMPLES']) if hparams['OFFSET_REP']=='max' else int(hparams['OFFSET_REP'])\n",
        "        offset = 0\n",
        "        total_loss = 0\n",
        "        for rep in range(reps):\n",
        "            v, dataset = reinitialize_vocab(v, hparams, credentials, offset, verbose=True)\n",
        "\n",
        "            for (batch, (inp, tar)) in enumerate(dataset.take(N_BATCH)):\n",
        "                batch_time = time.time()\n",
        "                batch_loss = train_step(hparams, inp, tar, h1, h2)\n",
        "                ep_losses.append(batch_loss)\n",
        "                total_loss += batch_loss\n",
        "\n",
        "                if batch % print_step == 0 or batch == 0:\n",
        "                  print('  >>> Epoch: {} | Batch: {}\\\\{} | Loss: {:.4f} | Time: {:.2f} sec'\n",
        "                      .format(epoch, batch+1, N_BATCH, batch_loss, time.time() - batch_time))\n",
        "            \n",
        "            offset += hparams['NUM_EXAMPLES']\n",
        "            tf.keras.backend.clear_session()\n",
        "            print(' -> Rep: {} done!'.format(rep + 1))\n",
        "\n",
        "        sentences = random.choices(test_sentences, k=2)\n",
        "        result1, text1, _ = evaluate(sentences[0], v, enc, dec, hparams['MAX_LEN'])\n",
        "        result2, text2, _ = evaluate(sentences[1], v, enc, dec, hparams['MAX_LEN'])\n",
        "        print(50*'+')\n",
        "        print(text1)\n",
        "        print(result1)\n",
        "        print(text2)\n",
        "        print(result2)\n",
        "        print(50*'+')\n",
        "\n",
        "        if saving:\n",
        "            print('Saving model...')\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "            save_plot(ckpt_path, ep_losses)\n",
        "\n",
        "        plt_loss.append(total_loss/N_BATCH)\n",
        "        print('Epoch: {} | Loss: {:.4f} | Time: {:.2f} min'.format(epoch, total_loss/(N_BATCH*reps), (time.time()-epoch_time)/60))\n",
        "    return plt_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn__bw2wl6yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = load_hyper_params(hparams_path)\n",
        "credentials = service_account.Credentials.from_service_account_file(hparams['CREDENTIALS_PATH'])\n",
        "enc, dec, opt = create_model(hparams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6OgjZ-Va5P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#'''\n",
        "v = Vocabulary(max_len=hparams['MAX_LEN'])\n",
        "v.load_bigquery_vocab_from_indexed(credentials, hparams['VOCAB_DB'], hparams['VOCAB'], True)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=opt, encoder=enc, decoder=dec)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(ckpt_path))\n",
        "#checkpoint.restore(drive_main_path + 'checkpoints-final-1/' + 'ckpt-2')\n",
        "\n",
        "result, _, _ = evaluate(u'The world is changing once again!', v, enc, dec, hparams['MAX_LEN'])\n",
        "print(result)\n",
        "#'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y74Ypkl0mTvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isdir('./__pycache__/'): shutil.rmtree(path='./__pycache__/', ignore_errors=True, onerror=None)\n",
        "\n",
        "if hparams['TRAINING_MODE'] == 'single': plt_loss = train(hparams, credentials,500, initial_epoch=4, saving=True)\n",
        "elif hparams['TRAINING_MODE'] == 'multi': plt_loss = multi_initializer_train(hparams, credentials, 500, initial_epoch=4, saving=True)\n",
        "else: raise Exception('Please enter a valid TRAINING_MODE: \\'single\\' or \\'multi\\' '\n",
        "                      '(for \\'multi\\' please use OFFSET_REP >= 2 or \\'max\\')')\n",
        "\n",
        "plt.plot(plt_loss)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}