{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN/XWg5XPR7/xb1/VW2raXp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arsuh/Seq2Seq-Chatbot/blob/master/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fWCr_pub6OD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vagRSi5d07Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Arsuh/Seq2Seq-Chatbot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp51V6ct029d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.oauth2 import service_account\n",
        "from google.cloud import bigquery\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doI84Ngu1A-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive_main_path = '/content/drive/My Drive/Colab Files/Chatbot/'\n",
        "main_path = '/content/Seq2Seq-Chatbot/'\n",
        "hparams_path = main_path + 'hyper_parameters_std.json'\n",
        "#hparams_path = main_path + 'hyper_parameters_test.json'\n",
        "credentials_path = main_path + 'credentials.json'\n",
        "ckpt_path = drive_main_path + 'ckeckpoints/'\n",
        "ckpt_prefix = os.path.join(ckpt_path, 'ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2MPT8YX1MoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    PAD = '<PAD>'  # INDEX: 0\n",
        "    SOS = '<SOS>'  # INDEX: 1\n",
        "    EOS = '<EOS>'  # INDEX: 2\n",
        "    UNK = '<UNK>'  # INDEX: 3\n",
        "    special_tokens = [PAD, SOS, EOS, UNK]\n",
        "\n",
        "    def __init__(self, max_len=150, dictionary_size=None):\n",
        "        self.max_len = max_len\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.word_occurrence = {}\n",
        "        self.current_index = 4\n",
        "        self.dict_size = dictionary_size\n",
        "\n",
        "        self.inp = []\n",
        "        self.tar = []\n",
        "        self.tokenized = False\n",
        "\n",
        "        for nr, word in enumerate(Vocabulary.special_tokens):\n",
        "            self.word2idx[word] = nr\n",
        "            self.idx2word[nr] = word\n",
        "\n",
        "# -------------------MAIN-FUNCTIONS-------------------------\n",
        "\n",
        "    def create_index(self, text, creating_indices=True):\n",
        "        if not isinstance(text, str):\n",
        "            text = ''.join(map(str, text))\n",
        "\n",
        "        text = text.strip()\n",
        "        for word in text.split(' '):\n",
        "            if word in self.word_occurrence:\n",
        "                self.word_occurrence[word] += 1\n",
        "            else:\n",
        "                self.word_occurrence[word] = 1\n",
        "\n",
        "        if self.dict_size != None:\n",
        "            self.sort_by_occurence()\n",
        "            self.remove_words()\n",
        "\n",
        "        if creating_indices:\n",
        "            for word in self.word_occurrence:\n",
        "                self.word2idx[word] = self.current_index\n",
        "                self.idx2word[self.current_index] = word\n",
        "                self.current_index += 1\n",
        "\n",
        "    def add_words(self, text):\n",
        "        for word in text:\n",
        "            if word in self.word_occurrence:\n",
        "                self.word2idx[word] = self.current_index\n",
        "                self.idx2word[self.current_index] = word\n",
        "                self.current_index += 1\n",
        "\n",
        "    def add_words_aux(self, text, aux_vocab):\n",
        "        for word in text.split(' '):\n",
        "            if word not in aux_vocab:\n",
        "                aux_vocab.append(word)\n",
        "                self.word2idx[word] = self.current_index\n",
        "                self.idx2word[self.current_index] = word\n",
        "                self.current_index += 1\n",
        "\n",
        "        return aux_vocab\n",
        "\n",
        "    def remove_words(self):\n",
        "        i = 0\n",
        "        rem = []\n",
        "        for word in self.word_occurrence:\n",
        "            i += 1\n",
        "            if i > self.dict_size:\n",
        "                rem.append(word)\n",
        "\n",
        "        for itm in rem:\n",
        "            del self.word_occurrence[itm]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.word_occurrence)\n",
        "\n",
        "    def word_exists(self, word):\n",
        "        return word in self.word_occurrence\n",
        "\n",
        "    def sort_by_occurence(self):\n",
        "        self.word_occurrence = {k: v for k, v in sorted(\n",
        "            self.word_occurrence.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "    def decode_text(self, enc_text, remove_borders=False):\n",
        "        dec_text = []\n",
        "        for idx in enc_text:\n",
        "            # dec_text.append(self.idx2word[idx])\n",
        "\n",
        "            if remove_borders:\n",
        "                if idx == 1 and idx == 2:\n",
        "                    continue\n",
        "\n",
        "            if idx in self.idx2word and idx != 0:\n",
        "                #dec_text += self.idx2word[idx] + ' '\n",
        "                dec_text.append(self.idx2word[idx])\n",
        "\n",
        "        return dec_text\n",
        "\n",
        "    def encode_text(self, dec_text):\n",
        "        if not isinstance(dec_text, str):\n",
        "            dec_text = ''.join(map(str, dec_text))\n",
        "\n",
        "        #dec_text = Vocabulary.punctuate_text(dec_text)\n",
        "        #dec_text = Vocabulary.normalize_text(dec_text)\n",
        "        dec_text = self.integrate_special_tokens(dec_text)\n",
        "\n",
        "        enc_text = []\n",
        "        for word in dec_text.split(' '):\n",
        "            if word in self.word_occurrence:\n",
        "                enc_text.append(self.word2idx[word])\n",
        "            elif word in Vocabulary.special_tokens:\n",
        "                enc_text.append(Vocabulary.special_tokens.index(word))\n",
        "\n",
        "        return enc_text\n",
        "\n",
        "    def pad_text(self, enc_text):\n",
        "        while len(enc_text) < self.max_len:\n",
        "            enc_text.append(0)  # PAD\n",
        "        return enc_text\n",
        "\n",
        "    def get_final_text(self, enc_text):\n",
        "        if len(enc_text) > self.max_len:\n",
        "            enc_text = enc_text[:self.max_len - 1]\n",
        "            enc_text.append(2)  # EOS\n",
        "        elif len(enc_text) < self.max_len:\n",
        "            enc_text = self.pad_text(enc_text)\n",
        "\n",
        "        return enc_text\n",
        "\n",
        "    def preproc(self, text):\n",
        "        text = Vocabulary.punctuate_text(text)\n",
        "        text = Vocabulary.normalize_text(text)\n",
        "        text = Vocabulary.normalize_numbers(text)\n",
        "        text = self.encode_text(text)\n",
        "        text = self.get_final_text(text)\n",
        "        return text\n",
        "\n",
        "    def tokenize_data(self):\n",
        "        if self.tokenized == False:\n",
        "            i = 0\n",
        "            for _ in self.inp:\n",
        "                enc_text = self.encode_text(self.inp[i])\n",
        "                self.inp[i] = self.get_final_text(enc_text)\n",
        "                enc_text = self.encode_text(self.tar[i])\n",
        "                self.tar[i] = self.get_final_text(enc_text)\n",
        "                i += 1\n",
        "            self.tokenized = True\n",
        "\n",
        "    def de_tokenize_data(self):\n",
        "        if self.tokenized == True:\n",
        "            i = 0\n",
        "            for _ in self.inp:\n",
        "                self.inp[i] = self.decode_text(self.inp[i])\n",
        "                self.tar[i] = self.decode_text(self.tar[i])\n",
        "\n",
        "                j = 0\n",
        "                while j < len(self.inp[i]):\n",
        "                    if self.inp[i][j] in Vocabulary.special_tokens[:-1]:\n",
        "                        del self.inp[i][j]\n",
        "                        j -= 1\n",
        "                    j += 1\n",
        "\n",
        "                j = 0\n",
        "                while j < len(self.tar[i]):\n",
        "                    if self.tar[i][j] in Vocabulary.special_tokens[:-1]:\n",
        "                        del self.tar[i][j]\n",
        "                        j -= 1\n",
        "                    j += 1\n",
        "                i += 1\n",
        "            self.tokenized = False\n",
        "\n",
        "    def print_data(self):\n",
        "        print(' >>> VOCAB_SIZE: {}\\n >>> CURENT INDEX: {}'.format(\n",
        "            self.dict_size, self.current_index))\n",
        "\n",
        "# ----------------SAVING/LOADING-DATA---------------------\n",
        "\n",
        "    def save_csv(self, path, save_index=True):\n",
        "        total_words = self.size()\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            if save_index:\n",
        "                f.write('word,index,occurrence\\n')\n",
        "                for i in range(4, total_words):\n",
        "                    word = self.idx2word[i]\n",
        "                    occurrence = self.word_occurrence[word]\n",
        "                    f.write('{},{},{}\\n'.format(word, i, occurrence))\n",
        "            else:\n",
        "                f.write('word,occurrence\\n')\n",
        "                for word in self.word_occurrence:\n",
        "                    f.write('{},{}\\n'.format(word, self.word_occurrence[word]))\n",
        "\n",
        "    @staticmethod\n",
        "    def load_csv(path, word_occ_only=False, limit=None, verbose=True):\n",
        "        v = Vocabulary()\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            i = 1\n",
        "            for line in f.readlines()[1:]:\n",
        "                text = line.split(',')\n",
        "                if word_occ_only == False:\n",
        "                    v.word2idx[text[0]] = text[1]\n",
        "                    v.idx2word[text[1]] = text[0]\n",
        "                    v.word_occurrence[text[0]] = int(text[2][:-1])\n",
        "                else:\n",
        "                    v.word_occurrence[text[0]] = int(text[1][:-1])\n",
        "\n",
        "                if limit != None and i > limit:\n",
        "                    break\n",
        "                if verbose and i % 200000 == 0:\n",
        "                    print('{} rows added to vocabulary!'.format(i))\n",
        "                i += 1\n",
        "\n",
        "        sz = v.size()\n",
        "        v.current_index = sz + 1\n",
        "        # vocabulary.dict_size = sz\n",
        "        return v\n",
        "\n",
        "    @staticmethod\n",
        "    def create_query(sql, credentials):\n",
        "        try:\n",
        "            client = bigquery.Client(\n",
        "                credentials=credentials, project=credentials.project_id,)\n",
        "\n",
        "            return client.query(sql).result()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    def load_bigquery_main_vocab(self, credentials, limit=None, verbose=True):\n",
        "        \"\"\" Used only for creating intitial vocabulary \"\"\"\n",
        "        query = 'SELECT parent, comment FROM `reddit-chatobot.Reddit_db.mainTable`'\n",
        "        if limit != None:\n",
        "            query += ' LIMIT {}'.format(limit)\n",
        "\n",
        "        i = 1\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        for row in rows:\n",
        "            parent = Vocabulary.punctuate_text(str(row.parent))\n",
        "            parent = Vocabulary.normalize_text(parent)\n",
        "            self.create_index(parent, creating_indices=False)\n",
        "            comment = Vocabulary.punctuate_text(str(row.comment))\n",
        "            comment = Vocabulary.normalize_text(comment)\n",
        "            self.create_index(comment, creating_indices=False)\n",
        "\n",
        "            if verbose and i % 200000 == 0:\n",
        "                print('   >>> {} rows done!'.format(i))\n",
        "            i += 1\n",
        "\n",
        "        sz = self.size()\n",
        "        self.current_index = sz + 1\n",
        "        # vocabulary.dict_size = sz\n",
        "        if verbose:\n",
        "            self.print_data()\n",
        "\n",
        "    def load_bigquery_main(self, credentials, limit=None, verbose=True):\n",
        "        query = 'SELECT parent, comment FROM `reddit-chatobot.Reddit_db.mainTable`'\n",
        "        if limit != None:\n",
        "            query += ' LIMIT {}'.format(limit)\n",
        "\n",
        "        i = 1\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        aux_vocab = []\n",
        "        for row in rows:\n",
        "            parent = Vocabulary.punctuate_text(str(row.parent))\n",
        "            parent = Vocabulary.normalize_text(parent)\n",
        "            parent = Vocabulary.normalize_numbers(parent)\n",
        "            aux_vocab = self.add_words_aux(parent, aux_vocab)\n",
        "\n",
        "            comment = Vocabulary.punctuate_text(str(row.comment))\n",
        "            comment = Vocabulary.normalize_text(comment)\n",
        "            comment = Vocabulary.normalize_numbers(comment)\n",
        "            aux_vocab = self.add_words_aux(comment, aux_vocab)\n",
        "\n",
        "            if verbose and i % 200000 == 0:\n",
        "                print('   >>> {} rows done!'.format(i))\n",
        "            i += 1\n",
        "\n",
        "        del aux_vocab\n",
        "\n",
        "    def load_bigquery_vocab(self, credentials, vocab='no_ap', limit=None, verbose=True):\n",
        "        if vocab == 'no_ap':\n",
        "            query = 'SELECT word, occurrence FROM `reddit-chatobot.Reddit_db.lim_vocab_no_ap`'\n",
        "        elif vocab == 'ap':\n",
        "            query = 'SELECT word, occurrence FROM `reddit-chatobot.Reddit_db.lim_vocab`'\n",
        "        else:\n",
        "            raise Exception('Unknown vocab argument! Use \\'no_ap\\' or \\'ap\\'!')\n",
        "\n",
        "        if limit != None:\n",
        "            query += ' LIMIT {}'.format(limit)\n",
        "\n",
        "        i = 1\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        try:\n",
        "            for row in rows:\n",
        "                word = str(row.word)\n",
        "                occ = int(row.occurrence)\n",
        "                self.word_occurrence[word] = occ\n",
        "\n",
        "                if verbose and i % 200000 == 0:\n",
        "                    print('   >>> {} rows done!'.format(i))\n",
        "                i += 1\n",
        "\n",
        "            if verbose:\n",
        "                print('Word occurrence created!')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    def load_bigquery_vocab_from_indexed(self, credentials, vocab='no_ap', limit=None, verbose=True):\n",
        "        if vocab == 'no_ap':\n",
        "            query = 'SELECT * FROM `reddit-chatobot.Reddit_db.vocab_no_ap_indexed`'\n",
        "        elif vocab == 'ap':\n",
        "            query = 'SELECT * FROM `reddit-chatobot.Reddit_db.vocab_ap_indexed`'\n",
        "        else:\n",
        "            raise Exception('Unknown vocab argument! Use \\'no_ap\\' or \\'ap\\'!')\n",
        "\n",
        "        if limit != None:\n",
        "            query += ' LIMIT {}'.format(limit)\n",
        "\n",
        "        i = 1\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        try:\n",
        "            for row in rows:\n",
        "                word = str(row.word)\n",
        "                occ = int(row.occurrence)\n",
        "                idx = int(row.idx)\n",
        "                self.word_occurrence[word] = occ\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "\n",
        "                if verbose and i % 200000 == 0:\n",
        "                    print('   >>> {} rows done!'.format(i))\n",
        "                i += 1\n",
        "\n",
        "            if verbose:\n",
        "                print('Word occurrence created!')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_bigquery_full(credentials, max_len=150, vocab='no_ap', limit_main=None, limit_vocab=None, verbose=True):\n",
        "        v = Vocabulary(max_len=max_len)\n",
        "        v.load_bigquery_main(credentials, limit_main, verbose)\n",
        "        v.load_bigquery_vocab(credentials, vocab, limit_vocab, verbose)\n",
        "        v.dict_size = v.size()\n",
        "\n",
        "        if verbose:\n",
        "            v.print_data()\n",
        "        return v\n",
        "\n",
        "    @staticmethod\n",
        "    def create_inputs(credentials, max_len=150, vocab='no_ap', limit_main=None, limit_vocab=None, verbose=True):\n",
        "        v = Vocabulary(max_len=max_len)\n",
        "        v.load_bigquery_vocab(credentials, vocab, limit_vocab, verbose)\n",
        "\n",
        "        query = 'SELECT * FROM `reddit-chatobot.Reddit_db.inputs`'\n",
        "        if limit_main != None:\n",
        "            query += ' LIMIT {}'.format(limit_main)\n",
        "\n",
        "        i = 1\n",
        "        text = set()\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        for row in rows:\n",
        "            parent = str(row.parent)\n",
        "            v.inp.append(parent)\n",
        "            for word in parent.split(' '):\n",
        "                text.add(word)\n",
        "\n",
        "            comment = str(row.comment)\n",
        "            v.tar.append(comment)\n",
        "            for word in comment.split(' '):\n",
        "                text.add(word)\n",
        "\n",
        "            if verbose and i % 1000000 == 0:\n",
        "                # os.system('clear')\n",
        "                print('   >>> Main: {} rows done!'.format(i))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        v.add_words(text)\n",
        "        del text\n",
        "\n",
        "        if verbose:\n",
        "            print('Main Loaded!')\n",
        "        return v\n",
        "\n",
        "    @staticmethod\n",
        "    def create_inputs_from_indexed(credentials, max_len=150, vocab='no_ap', limit_main=None, limit_vocab=None, verbose=True):\n",
        "        v = Vocabulary(max_len=max_len)\n",
        "        v.load_bigquery_vocab_from_indexed(\n",
        "            credentials, vocab, limit_vocab, verbose)\n",
        "\n",
        "        #query = 'SELECT * FROM `reddit-chatobot.Reddit_db.inputs` WHERE LENGTH(comment)>25 AND LENGTH(parent)>25'\n",
        "        query = 'SELECT * FROM `reddit-chatobot.Reddit_db.inputs`'\n",
        "        if limit_main != None:\n",
        "            query += ' LIMIT {}'.format(limit_main)\n",
        "\n",
        "        i = 1\n",
        "        rows = Vocabulary.create_query(query, credentials)\n",
        "        for row in rows:\n",
        "            parent = str(row.parent)\n",
        "            v.inp.append(parent)\n",
        "\n",
        "            comment = str(row.comment)\n",
        "            v.tar.append(comment)\n",
        "\n",
        "            if verbose and i % 1000000 == 0:\n",
        "                # os.system('clear')\n",
        "                print('   >>> Main: {} rows done!'.format(i))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        v.current_index = i\n",
        "\n",
        "        if verbose:\n",
        "            print('Main Loaded!')\n",
        "        return v\n",
        "\n",
        "\n",
        "# ---------------------PREPROCESSING------------------------------\n",
        "\n",
        "    def integrate_special_tokens(self, sentence):\n",
        "        #sentence = Vocabulary.punctuate_text(sentence)\n",
        "        #sentence = Vocabulary.normalize_text(sentence)\n",
        "        sentence = list(sentence.split(' '))\n",
        "        for i, word in enumerate(sentence):\n",
        "            if not self.word_exists(word):\n",
        "                sentence[i] = Vocabulary.UNK\n",
        "\n",
        "        sentence.insert(0, Vocabulary.SOS)\n",
        "        sentence.append(Vocabulary.EOS)\n",
        "        return ' '.join(sentence)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_numbers(text):\n",
        "        text = re.sub(r'[0]', ' 0 ', text)\n",
        "        text = re.sub(r'[1]', ' 1 ', text)\n",
        "        text = re.sub(r'[2]', ' 2 ', text)\n",
        "        text = re.sub(r'[3]', ' 3 ', text)\n",
        "        text = re.sub(r'[4]', ' 4 ', text)\n",
        "        text = re.sub(r'[5]', ' 5 ', text)\n",
        "        text = re.sub(r'[6]', ' 6 ', text)\n",
        "        text = re.sub(r'[7]', ' 7 ', text)\n",
        "        text = re.sub(r'[8]', ' 8 ', text)\n",
        "        text = re.sub(r'[9]', ' 9 ', text)\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_text(text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[’`]\", \"'\", text)\n",
        "        text = re.sub(r\"i'm\", \"i am\", text)\n",
        "        text = re.sub(r\"he's\", \"he is\", text)\n",
        "        text = re.sub(r\"she's\", \"she is\", text)\n",
        "        text = re.sub(r\"that's\", \"that is\", text)\n",
        "        text = re.sub(r\"there's\", \"there is\", text)\n",
        "        text = re.sub(r\"what's\", \"what is\", text)\n",
        "        text = re.sub(r\"where's\", \"where is\", text)\n",
        "        text = re.sub(r\"who's\", \"who is\", text)\n",
        "        text = re.sub(r\"how's\", \"how is\", text)\n",
        "        # text = re.sub(r\"it's\", \"it is\", text)           #<--- exception\n",
        "        text = re.sub(r\"let's\", \"let us\", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "        text = re.sub(r\"\\'re\", \" are\", text)\n",
        "        text = re.sub(r\"\\'d\", \" would\", text)\n",
        "        text = re.sub(r\"won't\", \"will not\", text)\n",
        "        text = re.sub(r\"shan't\", \"shall not\", text)\n",
        "        text = re.sub(r\"can't\", \"can not\", text)\n",
        "        text = re.sub(r\"cannot\", \"can not\", text)\n",
        "        text = re.sub(r\"n't\", \" not\", text)\n",
        "\n",
        "        text = re.sub(r\"[@\\\\/|~_&#=+`$*,^]\", \"\", text)\n",
        "        text = re.sub(r\"[.]+\", \" . \", text)\n",
        "        text = re.sub(r\"[!]+\", \" ! \", text)\n",
        "        text = re.sub(r\"[?]+\", \" ? \", text)\n",
        "        text = re.sub(r\"[-]+\", \" \", text)\n",
        "        text = re.sub(r\"[(<{\\[]\", \" ( \", text)\n",
        "        text = re.sub(r\"[)>}\\]]\", \" ) \", text)\n",
        "        text = re.sub(r'[\"“”]', ' \" ', text)\n",
        "        text = re.sub(r\"[:]\", \" : \", text)\n",
        "        text = re.sub(r\"[;]\", \" ; \", text)\n",
        "        text = re.sub(r\"[%]\", \" % \", text)\n",
        "        text = re.sub(r\"[\\t\\r]+\", \" \", text)\n",
        "        text = re.sub(r\"[\\n\\t\\r]\", \"\", text)\n",
        "        text = re.sub(r\" +\", \" \", text).strip()\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def punctuate_text(text):\n",
        "        text = text.strip()\n",
        "        if not (text.endswith(\".\") or text.endswith(\"?\") or text.endswith(\"!\")):\n",
        "            tmp = re.sub(r\"'\", '\"', text.lower())\n",
        "            if (tmp.startswith(\"who\") or tmp.startswith(\"what\") or tmp.startswith(\"when\") or\n",
        "                    tmp.startswith(\"where\") or tmp.startswith(\"why\") or tmp.startswith(\"how\") or\n",
        "                    tmp.endswith(\"who\") or tmp.endswith(\"what\") or tmp.endswith(\"when\") or\n",
        "                    tmp.endswith(\"where\") or tmp.endswith(\"why\") or tmp.endswith(\"how\") or\n",
        "                    tmp.startswith(\"are\") or tmp.startswith(\"will\") or tmp.startswith(\"wont\") or tmp.startswith(\"can\")):\n",
        "                text = \"{} ? \".format(text)\n",
        "            else:\n",
        "                text = \"{} . \".format(text)\n",
        "        return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmzMZwxy1Q_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMB = 256\n",
        "RNN1 = 512\n",
        "RNN2 = 512\n",
        "RNN_TYPE = 'lstm'\n",
        "DROPOUT = 0.2\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_sz, batch_sz,\n",
        "                 _emb=EMB, _rnn1=RNN1, _rnn2=RNN2, _rnn_type=RNN_TYPE, bidirectional=True, _merge_mode='ave', _dr=DROPOUT):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.vocab_sz = vocab_sz + 4\n",
        "        self.batch_sz = batch_sz\n",
        "        self.construct_model(_emb, _rnn1, _rnn2, _rnn_type,\n",
        "                             bidirectional, _merge_mode, _dr)\n",
        "        # self.build(tf.TensorShape([self.batch_sz, None]))\n",
        "\n",
        "    def construct_model(self, _emb, _rnn1, _rnn2, _rnn_type, bidirectional, _merge_mode, _dr):\n",
        "        self.emb_dim = _emb\n",
        "        self.rnn1_units = _rnn1\n",
        "        self.rnn2_units = _rnn2\n",
        "        self.rnn_type = _rnn_type\n",
        "        self.bidirectional = bidirectional\n",
        "        if self.bidirectional == False:\n",
        "            self._merge_mode = None\n",
        "        else:\n",
        "            self._merge_mode = _merge_mode\n",
        "        self.dropout = _dr\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_sz, self.emb_dim,\n",
        "                                                   batch_input_shape=[self.batch_sz, None],)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            if self.rnn_type == 'gru':\n",
        "                rnn1_forward = gru_fnc(self.rnn1_units, self.dropout)\n",
        "                rnn1_backward = gru_fnc(\n",
        "                    self.rnn1_units, self.dropout, inverse=True)\n",
        "                rnn2_forward = gru_fnc(self.rnn2_units, self.dropout)\n",
        "                rnn2_backward = gru_fnc(\n",
        "                    self.rnn2_units, self.dropout, inverse=True)\n",
        "            elif self.rnn_type == 'lstm':\n",
        "                rnn1_forward = lstm_fnc(self.rnn1_units, self.dropout)\n",
        "                rnn1_backward = lstm_fnc(\n",
        "                    self.rnn1_units, self.dropout, inverse=True)\n",
        "                rnn2_forward = lstm_fnc(self.rnn2_units, self.dropout)\n",
        "                rnn2_backward = lstm_fnc(\n",
        "                    self.rnn2_units, self.dropout, inverse=True)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    'RNN TYPE not recognized! Please use \\'gru\\' or \\'lstm\\'!')\n",
        "\n",
        "            self.rnn1 = tf.keras.layers.Bidirectional(\n",
        "                rnn1_forward, backward_layer=rnn1_backward, merge_mode=self._merge_mode)\n",
        "            self.rnn2 = tf.keras.layers.Bidirectional(\n",
        "                rnn2_forward, backward_layer=rnn2_backward, merge_mode=self._merge_mode)\n",
        "        else:\n",
        "            if self.rnn_type == 'gru':\n",
        "                self.rnn1 = gru_fnc(self.rnn1_units, self.dropout)\n",
        "                self.rnn2 = gru_fnc(self.rnn2_units, self.dropout)\n",
        "            elif self.rnn_type == 'lstm':\n",
        "                self.rnn1 = lstm_fnc(self.rnn1_units, self.dropout)\n",
        "                self.rnn2 = lstm_fnc(self.rnn2_units, self.dropout)\n",
        "            else:\n",
        "                raise Exception(\n",
        "                    'RNN TYPE not recognized! Please use \\'gru\\' or \\'lstm\\'!')\n",
        "\n",
        "        self.W = tf.keras.layers.Dense(self.rnn1_units)\n",
        "\n",
        "    def call(self, x, h1, h2, training=True):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        if self.rnn_type == 'gru' and self.bidirectional == False:\n",
        "            output1, state1 = self.rnn1(x, initial_state=h1, training=training)\n",
        "            output2, state2 = self.rnn2(\n",
        "                self.W(output1), initial_state=h2, training=training)\n",
        "            return output2, state1, state2\n",
        "\n",
        "        output1 = self.rnn1(x, initial_state=h1, training=training)\n",
        "        output2 = self.rnn2(\n",
        "            self.W(output1[0]), initial_state=h2, training=training)\n",
        "        # result, h1, h2\n",
        "        return output2[0], output1[1] + output1[2], output2[1] + output2[2]\n",
        "\n",
        "    def initialize_hidden(self, batch=None):\n",
        "        if batch is None:\n",
        "            batch = self.batch_sz\n",
        "\n",
        "        nr_gru, nr_lstm = 2, 4\n",
        "        if self.bidirectional == False:\n",
        "            nr_gru = nr_gru // 2\n",
        "            nr_lstm = nr_lstm // 2\n",
        "\n",
        "        if self.rnn_type == 'gru':\n",
        "            h1 = [tf.zeros((batch, self.rnn1_units)) for _ in range(nr_gru)]\n",
        "            h2 = [tf.zeros((batch, self.rnn2_units)) for _ in range(nr_gru)]\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            h1 = [tf.zeros((batch, self.rnn1_units)) for _ in range(nr_lstm)]\n",
        "            h2 = [tf.zeros((batch, self.rnn2_units)) for _ in range(nr_lstm)]\n",
        "        return h1, h2\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_sz, batch_sz,\n",
        "                 _emb=EMB, _rnn1=RNN1, _rnn2=RNN2, _rnn_type=RNN_TYPE, _dr=DROPOUT):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.vocab_sz = vocab_sz + 4\n",
        "        self.batch_sz = batch_sz\n",
        "\n",
        "        self.construct_model(_emb, _rnn1, _rnn2, _rnn_type, _dr)\n",
        "        # self.build(tf.TensorShape([self.batch_sz, None]))\n",
        "\n",
        "    def construct_model(self, _emb, _rnn1, _rnn2, _rnn_type, _dr):\n",
        "        self.emb_dim = _emb\n",
        "        self.rnn1_units = _rnn1\n",
        "        self.rnn2_units = _rnn2\n",
        "        self.rnn_type = _rnn_type\n",
        "        self.dropout = _dr\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(self.vocab_sz, self.emb_dim,\n",
        "                                                   batch_input_shape=[self.batch_sz, None])\n",
        "\n",
        "        if self.rnn_type == 'gru':\n",
        "            self.rnn1 = gru_fnc(self.rnn1_units, self.dropout)\n",
        "            self.rnn2 = gru_fnc(self.rnn2_units, self.dropout)\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            self.rnn1 = lstm_fnc(self.rnn1_units, self.dropout)\n",
        "            self.rnn2 = lstm_fnc(self.rnn2_units, self.dropout)\n",
        "        else:\n",
        "            raise Exception(\n",
        "                'RNN TYPE not recognized! Plase use \\'gru\\' or \\'lstm\\'!')\n",
        "\n",
        "        self.W = tf.keras.layers.Dense(self.rnn1_units)\n",
        "        self.attention = BahdanauAttention(\n",
        "            self.batch_sz, self.rnn1_units, self.rnn2_units)\n",
        "        self.fc = tf.keras.layers.Dense(self.vocab_sz)\n",
        "        #self.final = tf.keras.layers.Dense(self.vocab_sz, activation='softmax')\n",
        "\n",
        "    def call(self, x, enc_output, h1, h2, training=True):\n",
        "        context_vector, attention_weights = self.attention(enc_output, h1, h2)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
        "\n",
        "        output1 = self.rnn1(x, training=training)\n",
        "        output2 = self.rnn2(self.W(output1[0]), training=training)\n",
        "        state2 = output2[1]\n",
        "        output = output2[0]\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        x = self.fc(output)\n",
        "        #x = self.final(x)\n",
        "        # result, h1, h2, attention_wieghts\n",
        "        return x, output1[1], state2, attention_weights\n",
        "\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, batch_sz, rnn1_units, rnn2_units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W_enc = tf.keras.layers.Dense(rnn2_units)  # <--- Encoder output\n",
        "        self.W1 = tf.keras.layers.Dense(rnn1_units)  # <--- Hidden state 1\n",
        "        self.W2 = tf.keras.layers.Dense(rnn2_units)  # <--- Hidden state 2\n",
        "        self.W = tf.keras.layers.Dense(1)      # <--- Score\n",
        "\n",
        "        # self.build(tf.TensorShape([batch_sz, None]))\n",
        "\n",
        "    def call(self, output, h1, h2):\n",
        "        h1_expand = tf.expand_dims(h1, axis=1)\n",
        "        h2_expand = tf.expand_dims(h2, axis=1)\n",
        "\n",
        "        x = self.W_enc(output)\n",
        "        concat_rnns = tf.concat(\n",
        "            (self.W2(h2_expand), self.W1(h1_expand)), axis=2)\n",
        "        x = BahdanauAttention.broadcast(x, concat_rnns.shape)\n",
        "\n",
        "        score = self.W(tf.tanh(x + concat_rnns))\n",
        "\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = output * attention_weights\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "    @staticmethod\n",
        "    def broadcast(tensor, shape):\n",
        "        return tf.concat((tensor, tf.zeros([tensor.shape[0], tensor.shape[1], shape[2]-tensor.shape[2]])), axis=2)\n",
        "\n",
        "\n",
        "def gru_fnc(units, dropout, inverse=False):\n",
        "    return tf.keras.layers.GRU(units,\n",
        "                               activation='tanh',\n",
        "                               recurrent_activation='sigmoid',\n",
        "                               recurrent_dropout=0.0,\n",
        "                               unroll=False,\n",
        "                               use_bias=True,\n",
        "                               # reset_after=True,\n",
        "                               return_sequences=True,\n",
        "                               return_state=True,\n",
        "                               stateful=False,\n",
        "                               recurrent_initializer='glorot_uniform',\n",
        "                               go_backwards=inverse,\n",
        "                               dropout=dropout,\n",
        "                               )\n",
        "\n",
        "\n",
        "def lstm_fnc(units, dropout, inverse=False):\n",
        "    return tf.keras.layers.LSTM(units,\n",
        "                                activation='tanh',\n",
        "                                recurrent_activation='sigmoid',\n",
        "                                recurrent_dropout=0.0,\n",
        "                                unroll=False,\n",
        "                                use_bias=True,\n",
        "                                # reset_after=True,\n",
        "                                return_sequences=True,\n",
        "                                return_state=True,\n",
        "                                stateful=False,\n",
        "                                recurrent_initializer='glorot_uniform',\n",
        "                                go_backwards=inverse,\n",
        "                                dropout=dropout,\n",
        "                                )\n",
        "\n",
        "\n",
        "_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "\n",
        "def loss_fnc(y_true, y_pred):\n",
        "    loss = _loss(y_true, y_pred)\n",
        "    mask = 1 - np.array_equal(y_true, 0)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    return tf.reduce_mean(loss * mask)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgvCVbjC1TNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_model(hparams, from_indexed=True, create_ds=True, de_tokenize=True, verbose=False):\n",
        "    start = time.time()\n",
        "    if from_indexed:\n",
        "        v = Vocabulary.create_inputs_from_indexed(service_account.Credentials.from_service_account_file(credentials_path),\n",
        "                                                  max_len=hparams['MAX_LEN'],\n",
        "                                                  vocab=hparams['VOCAB_DB'],\n",
        "                                                  limit_main=hparams['NUM_EXAMPLES'],\n",
        "                                                  limit_vocab=hparams['VOCAB'],\n",
        "                                                  verbose=True)  # <--- False\n",
        "    else:\n",
        "        v = Vocabulary.create_inputs(service_account.Credentials.from_service_account_file(credentials_path),\n",
        "                                     max_len=hparams['MAX_LEN'],\n",
        "                                     vocab=hparams['VOCAB_DB'],\n",
        "                                     limit_main=hparams['NUM_EXAMPLES'],\n",
        "                                     limit_vocab=hparams['VOCAB'],\n",
        "                                     verbose=True)  # <--- False\n",
        "\n",
        "    if de_tokenize:\n",
        "        v.de_tokenize_data()\n",
        "    if verbose:\n",
        "        print('Vocabulary created!')\n",
        "\n",
        "    if create_ds:\n",
        "        dataset = create_dataset(\n",
        "            v, hparams['BATCH_SIZE'], hparams['NUM_EXAMPLES'])\n",
        "    enc = Encoder(hparams['VOCAB'], hparams['BATCH_SIZE'], hparams['EMBEDDING'],\n",
        "                  hparams['RNN1'], hparams['RNN2'], hparams['RNN_TYPE'], hparams['BIDIRECTIONAL'], hparams['MERGE_MODE'], hparams['DROPOUT_ENC'])\n",
        "    dec = Decoder(hparams['VOCAB'], hparams['BATCH_SIZE'], hparams['EMBEDDING'],\n",
        "                  hparams['RNN1'], hparams['RNN2'], hparams['RNN_TYPE'], hparams['DROPOUT_DEC'])\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=hparams['LR'])\n",
        "    #opt = tf.keras.optimizers.SGD(learning_rate=hparams['LR'], momentum=0.5)\n",
        "\n",
        "    print('Time to initialize model {:.2f} min | {:.2f} hrs\\n'.format(\n",
        "        (time.time()-start)/60, (time.time()-start)/3600))\n",
        "\n",
        "    if create_ds:\n",
        "        return v, dataset, enc, dec, opt\n",
        "    return v, enc, dec, opt\n",
        "\n",
        "\n",
        "def load_hyper_params(path):\n",
        "    with open(path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for k in data:\n",
        "        if data[k] == 'None':\n",
        "            data[k] = None\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_dataset(v, batch_size, buffer_size):\n",
        "    v.tokenize_data()\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (np.array(v.inp, dtype=np.int32), np.array(v.tar, dtype=np.int32)))\n",
        "    dataset = dataset.shuffle(buffer_size)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def save_plot(path, plt_loss):\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)\n",
        "    with open(path + 'plot.txt', 'a', encoding='utf-8') as f:\n",
        "        f.write(str(plt_loss[-1].numpy()) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22-bc4uW1cVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(hparams, inp, tar, enc_h1, enc_h2):\n",
        "    global enc, dec, opt\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_out, enc_h1, enc_h2 = enc(inp, enc_h1, enc_h2)\n",
        "        dec_h1, dec_h2 = enc_h1, enc_h2\n",
        "        dec_inp = tf.expand_dims([1]*hparams['BATCH_SIZE'], 1)\n",
        "\n",
        "        for t in range(1, tar.shape[1]):\n",
        "            pred, dec_h1, dec_h2, _ = dec(dec_inp, enc_out, dec_h1, dec_h2)\n",
        "            #pred = tf.nn.softmax(pred, axis=-1)\n",
        "\n",
        "            loss += loss_fnc(tar[:, t], pred)\n",
        "            dec_inp = tf.expand_dims(tar[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss/int(tar.shape[1]))\n",
        "    variables = enc.trainable_variables + dec.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    opt.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "test_sentences = ['Hello!',\n",
        "                  'How are you?',\n",
        "                  'Tomorow is my birthday, but I keep feeling sad...',\n",
        "                  'What is your name sir?',\n",
        "                  'Artificial intelligence will take over the world some day!',\n",
        "                  'Can you please bing me some water?',\n",
        "                  'Come on! This is the easiest thing you are supposed to do!',\n",
        "                  'My name is Thomas!']\n",
        "\n",
        "\n",
        "def train(hparams, saving=True, saving_step=1, plot_saving=True, verbose=True):\n",
        "    global v, dataset, enc, dec, opt\n",
        "    if hparams['NUM_EXAMPLES'] == None:\n",
        "        N_BATCH = hparams['MAX_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "    else:\n",
        "        N_BATCH = hparams['NUM_EXAMPLES'] // hparams['BATCH_SIZE']\n",
        "\n",
        "    if saving:\n",
        "        checkpoint = tf.train.Checkpoint(\n",
        "            optimizer=opt, encoder=enc, decoder=dec)\n",
        "\n",
        "    plt_loss = []\n",
        "    for epoch in range(1, hparams['EPOCHS']+1):\n",
        "        h1, h2 = enc.initialize_hidden()\n",
        "\n",
        "        total_loss = 0\n",
        "        for (batch, (inp, tar)) in enumerate(dataset.take(N_BATCH)):\n",
        "            batch_time = time.time()\n",
        "            batch_loss = train_step(hparams, inp, tar, h1, h2)\n",
        "            total_loss += batch_loss\n",
        "\n",
        "            if batch % 30 == 0:\n",
        "              print('  >>> Epoch: {} | Batch: {}\\\\{} | Loss: {:.4f} | Time: {:.2f} sec'\n",
        "                    .format(epoch, batch+1, N_BATCH, batch_loss, time.time() - batch_time))\n",
        "\n",
        "        print('Epoch: {} | Loss: {:.4f}'.format(epoch+1, total_loss/N_BATCH))\n",
        "        plt_loss.append(total_loss/N_BATCH)\n",
        "\n",
        "        sentences = random.choices(test_sentences, k=2)\n",
        "        result1, text1, _ = evaluate(sentences[0], v, enc, dec)\n",
        "        result2, text2, _ = evaluate(sentences[1], v, enc, dec)\n",
        "        print(50*'+')\n",
        "        print(text1)\n",
        "        print(result1)\n",
        "        print(text2)\n",
        "        print(result2)\n",
        "        print(50*'+')\n",
        "\n",
        "        if saving and epoch%saving_step == 0:\n",
        "            print('Saving model...')\n",
        "            checkpoint.save(file_prefix=ckpt_prefix)\n",
        "        if plot_saving:\n",
        "            save_plot(ckpt_path, plt_loss)\n",
        "    return plt_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_43s-iVT1f9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(text, v, enc, dec):\n",
        "    inp = np.array(v.preproc(text), dtype=np.float32)\n",
        "    inp = tf.convert_to_tensor(inp)\n",
        "    inp = tf.expand_dims(inp, axis=0)\n",
        "\n",
        "    result = ''\n",
        "    h1, h2 = enc.initialize_hidden(batch=1)\n",
        "\n",
        "    enc_out, h1, h2 = enc.call(inp, h1, h2, training=False)\n",
        "    dec_inp = tf.expand_dims([1], axis=0)  # SOS\n",
        "    result += '<SOS> '\n",
        "    for _ in range(v.max_len):\n",
        "        pred, h1, h2, attention_wieghts = dec.call(\n",
        "            dec_inp, enc_out, h1, h2, training=False)\n",
        "\n",
        "        #pred = tf.nn.softmax(pred, axis=1)\n",
        "        pred_id = tf.argmax(pred[0]).numpy()\n",
        "        #pred_id = tf.random.categorical(pred, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        result += v.idx2word[pred_id] + ' '\n",
        "        if pred_id == 2:  # EOS\n",
        "            return result, text, attention_wieghts\n",
        "\n",
        "        dec_inp = tf.expand_dims([pred_id], axis=0)\n",
        "\n",
        "    return result[:-1], text, attention_wieghts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFJS2LU21iZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = load_hyper_params(hparams_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swrbayXc1njy",
        "colab_type": "code",
        "outputId": "a8404dec-cf79-4245-c0db-800773882a69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "v, dataset, enc, dec, opt = initialize_model(hparams, from_indexed=True, create_ds=True, de_tokenize=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word occurrence created!\n",
            "Main Loaded!\n",
            "Time to initialize model 0.34 min | 0.01 hrs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqFVEm0hPnkt",
        "colab_type": "code",
        "outputId": "3914b403-3c0e-4d45-b1f5-55b504ff1e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "checkpoint = tf.train.Checkpoint(optimizer=opt, encoder=enc, decoder=dec)\n",
        "checkpoint.restore(tf.train.latest_checkpoint(ckpt_path))\n",
        "\n",
        "\n",
        "result, _, _ = evaluate(u'The world is changing once again!', v, enc, dec)\n",
        "print(result)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncheckpoint = tf.train.Checkpoint(optimizer=opt, encoder=enc, decoder=dec)\\ncheckpoint.restore(tf.train.latest_checkpoint(ckpt_path))\\n\\n\\nresult, _, _ = evaluate(u'The world is changing once again!', v, enc, dec)\\nprint(result)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wl1WJft1pED",
        "colab_type": "code",
        "outputId": "df8083e0-5732-49f8-f0a2-d74f79b31f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "plt_loss = train(hparams, saving=True, saving_step=50, plot_saving=True)\n",
        "del dataset\n",
        "\n",
        "plt.plot(plt_loss)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  >>> Epoch: 1 | Batch: 1\\195 | Loss: 3.4686 | Time: 1.33 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}